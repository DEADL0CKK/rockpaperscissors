{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWL8eTGPmmxg"
   },
   "source": [
    "# Cat vs. Dog Image Classification\n",
    "## Exercise 1: Building a Convnet from Scratch\n",
    "\n",
    "In this exercise, we will build a classifier model from scratch that is able to distinguish dogs from cats. We will follow these steps:\n",
    "\n",
    "1. Explore the example data\n",
    "2. Build a small convnet from scratch to solve our classification problem\n",
    "3. Evaluate training and validation accuracy\n",
    "\n",
    "Let's go!\n",
    "\n",
    "\n",
    "## Explore the Example Data\n",
    "Let's start by downloading our example data, a .zip of 2,000 JPG pictures of cats and dogs, and extracting it locally in `/tmp`.  \n",
    "**NOTE:** The 2,000 images used in this exercise are excerpted from the [\"Dogs vs. Cats\" dataset](https://www.kaggle.com/c/dogs-vs-cats/data) available on Kaggle, which contains 25,000 images. Here, we use a subset of the full dataset to decrease training time for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_type = \"windows\"\n",
    "# os_type = \"linux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under ./tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop (1).h5\n"
     ]
    }
   ],
   "source": [
    "if os_type == \"linux\" :\n",
    "    !wget --no-check-certificate \\\n",
    "        https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
    "        -O ./tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
    "elif os_type == \"windows\" :\n",
    "    # Windows Version\n",
    "    !python -m wget https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 -o ./tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwWmLcW3nbjT",
    "outputId": "ffbfac2f-6e83-4433-c4af-9195cc2a4852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting zipfile36\n",
      "  Downloading zipfile36-0.1.3-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: zipfile36\n",
      "Successfully installed zipfile36-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install zipfile36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "L3BMtoDvmmxr",
    "outputId": "a75aae9c-7745-46b0-a0c0-24f63df76b11",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import zipfile36 as zipfile \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WtC-8g90mmxt"
   },
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "base_dir = './'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaKiSB5mmmxu",
    "outputId": "c56658a6-6dde-4e67-cc90-1f167d92a57c",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Le chemin d’accès spécifié est introuvable: 'train/paper/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's find out the total number of cat and dog images in the `train` and `validation` directories:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# os.listdir()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./rpc/train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain/paper/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/rock/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/scissors/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Le chemin d’accès spécifié est introuvable: 'train/paper/'"
     ]
    }
   ],
   "source": [
    "# Let's find out the total number of cat and dog images in the `train` and `validation` directories:\n",
    "\n",
    "# os.listdir()\n",
    "os.listdir('train')\n",
    "os.listdir('train/paper/')\n",
    "os.listdir('train/rock/')\n",
    "os.listdir('train/scissors/')\n",
    "\n",
    "print('total training paper images:', len(os.listdir('train/paper')))\n",
    "print('total training rock images:', len(os.listdir('train/rock')))\n",
    "print('total validation scissors images:', len(os.listdir('train//scissors')))\n",
    "print('total validation paper images:', len(os.listdir('validation/paper')))\n",
    "print('total validation rock images:', len(os.listdir('validation/rock')))\n",
    "print('total validation scissors images:', len(os.listdir('validation/scissors')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "RZ_4qks6mmxv",
    "outputId": "d2cb2155-6143-41af-8a93-305714933ad8"
   },
   "outputs": [],
   "source": [
    "# Retourner ds 1 tableau numpy les pixels de l'image\n",
    "paper1 = imread('./train/paper/paper01-052.png')\n",
    "# train_dir+'/paper'+'/cat.0.jpg'\n",
    "# paper1\n",
    "# cat1.shape  #  shape  => les dimensions\n",
    "plt.imshow(paper1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "m8D4xa7cmmxx",
    "outputId": "358ff2e7-d3f8-435c-aa8a-96807d07bfb0"
   },
   "outputs": [],
   "source": [
    "# For both cats and dogs, we have 1,000 training images and 500 test images.\n",
    "\n",
    "# Now let's take a look at a few pictures to get a better sense of what the cat and dog datasets look like.\n",
    "\n",
    "# Une boucle pr afficher plusieurs images dans une figure\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    filename = 'validation/' + 'paper/' + 'paper' + str(i+1) + '.png'\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "    plt.title(image.shape)\n",
    "    plt.axis('Off') # Don't show axes (or gridlines)\n",
    "filename\n",
    "# S : On constate que les images sont en mode paysage, mode portrait et de différentes tailles  \n",
    "# => Ceci peut poser des Pb pour la perfermonce du classifieur. Il nous faut un classifieur robuste !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "uqXjQgs9mmxz",
    "outputId": "2e6d2f4d-cd2a-4f88-843f-59c163a4257d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    filename = 'validation/' + 'rock/' + 'rock' + str(i+1) + '.png'\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "    plt.title(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    filename = 'validation/' + 'scissors/' + 'scissors' + str(i+1) + '.png'\n",
    "    image = imread(filename)\n",
    "    plt.imshow(image)\n",
    "    plt.title(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6EE3WAjWmmx0",
    "outputId": "c0a4285a-5eaa-4f12-f125-2db9a92e2bd9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Manip avec PIL : pkg Python de réf pr le traitement d'image\n",
    "paper1 = Image.open('train/paper/paper01-001.png')\n",
    "# paper1\n",
    "# paper1.show()\n",
    "# paper1.size\n",
    "# paper1.mode\n",
    "# paper1.format\n",
    "# np.array(paper1)\n",
    "# imread('train/paper/paper01-001.png')\n",
    "\n",
    "# paper1 = tf.keras.preprocessing.image.load_img('train/paper/paper01-001.png')\n",
    "# paper1\n",
    "paper1_ = tf.keras.preprocessing.image.img_to_array(paper1)\n",
    "paper1_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "JrvDoTS5mmx1",
    "outputId": "dceb4cbd-64fd-45a8-84d9-4b3bb435c34f"
   },
   "outputs": [],
   "source": [
    "paper1sz = paper1.resize((150, 150))\n",
    "# paper1sz\n",
    "# paper1sz_ = tf.keras.preprocessing.image.img_to_array(paper1sz)\n",
    "# paper1sz_\n",
    "# paper1sz_sc = paper1sz_ * 1./255\n",
    "# paper1sz_\n",
    "tf.keras.preprocessing.image.array_to_img(paper1sz_sc)\n",
    "plt.imshow(paper1sz_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "qUnoynaUmmx2",
    "outputId": "38e5c07e-ae27-4d32-c8e9-b84b9fa318e9"
   },
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.image.load_img('train/paper/paper01-003.png', target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xrsXQ7Mmmx2"
   },
   "outputs": [],
   "source": [
    "# Q : Comment  injecter les images depuis l'arborescence des dossiers sources dans le CNN (sans passer par des étapes manuelles - preprocessing : normalisation, resizing, augmentation, ...) ?\n",
    "\n",
    "# R : Ns devons construire 'train_datagen' et 'val_datagen' à l'aide de tf.keras.preprocessing.image.ImageDataGenerator\n",
    "\n",
    "# Explication :\n",
    "# Cette classe permet de :\n",
    "\t# i) lire les images depuis l'arborescence,\n",
    "\t# ii) Convertir les images en tensors float32,\n",
    "\t# iii) Injecter avec les labels associés ds le CNN suivant un batch (par ex 20) après les avoir resizer (taille 150x150).\n",
    "\n",
    "\n",
    "# Par la suite, nous pouvons utiliser les méthodes .flow(), .flow_from_directory(), .flow_from_dataframe()\n",
    "# Ces generators peuvent être utilisés dans les modeles convolutifs : fit_generator, evaluate_generator, predict_generator\n",
    "\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pjTgEoQmmx3",
    "outputId": "02b700f9-aaee-461b-b504-e93d76f48144"
   },
   "outputs": [],
   "source": [
    "train_gen = train_datagen.flow_from_directory(\n",
    "    'train', # path du train \n",
    "    target_size=(150, 150), \n",
    "    batch_size=3, \n",
    "    class_mode='categorical', \n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "loZ4B0Lpmmx3",
    "outputId": "eb9fe408-d8db-4da1-8c71-0707c97e15ca"
   },
   "outputs": [],
   "source": [
    "# train_gen.n \t# nb d'images\n",
    "# S : 2000\n",
    "# train_gen.class_indices\n",
    "# {'paper': 0, 'rock': 1,'scissors': 2}\n",
    "# val_gen.classes \n",
    "# train_gen.labels \t# cette info est la plus intéressante parce que ns allons l'utiliser pr construire la matrice de confusion\n",
    "# train_gen.labels.shape \t# cette info est la plus intéressante parce que ns allons l'utiliser pr construire la matrice de confusion\n",
    "# array([0, 0, 0, ..., 1, 1, 1])\n",
    "\n",
    "# train_gen.batch_size \t# nb d'images à envoyer (lot)\n",
    "# 20\n",
    "\n",
    "train_gen.directory\n",
    "train_gen.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RV7l6aCqmmx4",
    "outputId": "dbf64997-3064-48a8-846f-e6b75afda54e"
   },
   "outputs": [],
   "source": [
    "val_gen = val_datagen.flow_from_directory(\n",
    "    'validation', \n",
    "    target_size=(150, 150), \n",
    "    batch_size=3, \n",
    "    class_mode='categorical',\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dSDpMeHmmx4",
    "outputId": "4cda446c-94a2-4da7-ebcc-3af218ce562a"
   },
   "outputs": [],
   "source": [
    "type(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WY4FQyT2mmx5",
    "outputId": "0d90d308-496e-4c92-87fd-5b718c3c0e82"
   },
   "outputs": [],
   "source": [
    "train_batches = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n",
    "    .flow_from_directory(directory='train', target_size=(224,224), class_mode='categorical', batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxasxyK6mmx5"
   },
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvGTmIeOmmx6"
   },
   "outputs": [],
   "source": [
    "# Visualiser les images resizés et rescalés\n",
    "imgs, labels = next(train_batches)\n",
    "# labels\n",
    "imgs\n",
    "# len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ax9u4xZDmmx6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotImages(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCuD2u4Fmmx6",
    "outputId": "19cbf7f0-683b-487a-e5aa-a3eadee9a899"
   },
   "outputs": [],
   "source": [
    "# Visualiser les images resizés et rescalés\n",
    "imgs, labels = next(train_gen)\n",
    "# labels\n",
    "# imgs\n",
    "len(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOAHC2Edmmx6",
    "outputId": "1a6f2d91-4287-430b-d4ab-2c70a428491c"
   },
   "outputs": [],
   "source": [
    "plotImages(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hN2FdQRmmx7",
    "outputId": "1af7f1ee-6c85-461b-b543-0daef4f8a613"
   },
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUh7JpRhmmx7"
   },
   "source": [
    "# Building a Small Convnet from Scratch to Get to 72% Accuracy\n",
    "\n",
    "The images that will go into our convnet are 150x150 color images.\n",
    "\n",
    "Let's code up the architecture. We will stack 3 {convolution + relu + maxpooling} modules. Our convolutions operate on 3x3 windows and our maxpooling layers operate on 2x2 windows. Our first convolution extracts 16 filters, the following one extracts 32 filters, and the last one extracts 64 filters.\n",
    "\n",
    "**NOTE**: This is a configuration that is widely used and known to work well for image classification. Also, since we have relatively few training examples (1,000), using just three convolutional modules keeps the model small, which lowers the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAmNeEFrmmx7"
   },
   "outputs": [],
   "source": [
    "# Ds le cadre du df data2 : x1, x2 comme features\n",
    "inputs = tf.keras.layers.Input(shape=(2, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljBjy5mrmmx7",
    "outputId": "02d3aee2-57db-4f60-fef9-600e0e1d5af1"
   },
   "outputs": [],
   "source": [
    "# Maintenant, ns travaillons sur des images, ns indiquons \n",
    "# les dim de l'image => (heigth, width, mode)\n",
    "# del model\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcFCsT7Zmmx8"
   },
   "source": [
    "We'll configure the specifications for model training. We will train our model with the `binary_crossentropy` loss, because it's a binary classification problem and our final activation is a sigmoid.\n",
    "\n",
    "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent]  because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpI6-Y4hmmx8"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], \n",
    "              optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBldXd2mmmx8",
    "outputId": "eaf3a349-d7b8-4083-9788-502eb0e5a340",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_gen, epochs=2, steps_per_epoch=100, \n",
    "                   validation_data=val_gen, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEKHok4nmmx8"
   },
   "outputs": [],
   "source": [
    "def diagnostic_learning_curves():\n",
    "  # Retrieve a list of accuracy results on training and validation data\n",
    "  # sets for each training epoch\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "\n",
    "  # Retrieve a list of list results on training and validation data\n",
    "  # sets for each training epoch\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  # Get number of epochs\n",
    "  epochs = range(len(acc))\n",
    "\n",
    "  # Plot training and validation accuracy per epoch\n",
    "  plt.plot(epochs, acc)\n",
    "  plt.plot(epochs, val_acc)\n",
    "  plt.title('Training and validation accuracy')\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  # Plot training and validation loss per epoch\n",
    "  plt.plot(epochs, loss)\n",
    "  plt.plot(epochs, val_loss)\n",
    "  plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "2txoCV2RmlzW",
    "outputId": "988c5771-1321-476d-9494-024894536211"
   },
   "outputs": [],
   "source": [
    "diagnostic_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nwi8D9ANmmx8"
   },
   "source": [
    "As you can see, we are **overfitting** like it's getting out of fashion. Our training accuracy (in blue) gets close to 100% (!) while our validation accuracy (in green) stalls as 70%. Our validation loss reaches its minimum after only five epochs.\n",
    "\n",
    "Since we have a relatively small number of training examples (2000), overfitting should be our number one concern. Overfitting happens when a model exposed to too few examples learns patterns that do not generalize to new data, i.e. when the model starts using irrelevant features for making predictions. For instance, if you, as a human, only see three images of people who are lumberjacks, and three images of people who are sailors, and among them the only person wearing a cap is a lumberjack, you might start thinking that wearing a cap is a sign of being a lumberjack as opposed to a sailor. You would then make a pretty lousy lumberjack/sailor classifier.\n",
    "\n",
    "Overfitting is the central problem in machine learning: given that we are fitting the parameters of our model to a given dataset, how can we make sure that the representations learned by the model will be applicable to data never seen before? How do we avoid learning things that are specific to the training data?\n",
    "\n",
    "In the next exercise, we'll look at ways to prevent overfitting in the cat vs. dog classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gg9-Ltbmmx9"
   },
   "source": [
    "### Visualizing Intermediate Representations\n",
    "\n",
    "To get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n",
    "\n",
    "Let's pick a random cat or dog image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "G5peuhtImmx9",
    "outputId": "36a6dfb0-307a-41a4-c2fe-28e9db193318"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Let's define a new Model that will take an image as input, and will output\n",
    "# intermediate representations for all layers in the previous model after\n",
    "# the first.\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "visualization_model = Model(inputs, successive_outputs)\n",
    "\n",
    "# Let's prepare a random input image of a cat or dog from the training set.\n",
    "cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
    "dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
    "img_path = random.choice(cat_img_files + dog_img_files)\n",
    "\n",
    "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Rescale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Let's run our image through our network, thus obtaining all\n",
    "# intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "# These are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "# Now let's display our representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  if len(feature_map.shape) == 4:\n",
    "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "    # The feature map has shape (1, size, size, n_features)\n",
    "    size = feature_map.shape[1]\n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    for i in range(n_features):\n",
    "      # Postprocess the feature to make it visually palatable\n",
    "      x = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std()\n",
    "      x *= 64\n",
    "      x += 128\n",
    "      x = np.clip(x, 0, 255).astype('uint8')\n",
    "      # We'll tile each filter into this big horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x\n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure(figsize=(scale * n_features, scale))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzl8omqVmmx9"
   },
   "source": [
    "As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called \"sparsity.\" Representation sparsity is a key feature of deep learning.\n",
    "\n",
    "\n",
    "These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj5_oHEommx9"
   },
   "source": [
    "# Cat vs. Dog Image Classification\n",
    "## Exercise 2: Reducing Overfitting\n",
    "**_Estimated completion time: 30 minutes_**\n",
    "\n",
    "In this notebook we will build on the model we created in Exercise 1 to classify cats vs. dogs, and improve accuracy by employing a couple strategies to reduce overfitting: **data augmentation** and **dropout**. \n",
    "\n",
    "We will follow these steps:\n",
    "\n",
    "1. Explore how data augmentation works by making random transformations to training images.\n",
    "2. Add data augmentation to our data preprocessing.\n",
    "3. Add dropout to the convnet.\n",
    "4. Retrain the model and evaluate loss and accuracy. \n",
    "\n",
    "Let's get started!  \n",
    "\n",
    "## Exploring Data Augmentation\n",
    "\n",
    "Let's get familiar with the concept of **data augmentation**, an essential way to fight overfitting for computer vision models.\n",
    "\n",
    "In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that at training time, **our model will never see the exact same picture twice**. This helps prevent overfitting and helps the model generalize better.\n",
    "\n",
    "This can be done by configuring a number of random transformations to be performed on the images read by our `ImageDataGenerator` instance. Let's get started with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvdCN_11mmx-"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0D3lkDAmmx-"
   },
   "source": [
    "These are just a few of the options available (for more, see the [Keras documentation](https://keras.io/preprocessing/image/). Let's quickly go over what we just wrote:\n",
    "\n",
    "- `rotation_range` is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
    "- `width_shift` and `height_shift` are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
    "- `shear_range` is for randomly applying shearing transformations.\n",
    "- `zoom_range` is for randomly zooming inside pictures.\n",
    "- `horizontal_flip` is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
    "- `fill_mode` is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
    "\n",
    "Let's take a look at our augmented images. First let's set up our example files, as in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P0MH2RFbmmx_",
    "outputId": "31c3b73b-4d39-4610-b58a-4058ed2621a1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "# img_path = os.path.join(train_cats_dir, train_cat_fnames[2])\n",
    "img = load_img('train/cats/cat.2.jpg', target_size=(150, 150))  # this is a PIL image\n",
    "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "  plt.figure(i)\n",
    "  imgplot = plt.imshow(array_to_img(batch[0]))\n",
    "  i += 1\n",
    "  if i % 5 == 0:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqApd20Gmmx_"
   },
   "source": [
    "## Add Data Augmentation to the Preprocessing Step\n",
    "\n",
    "Now let's add our data-augmentation transformations from [**Exploring Data Augmentation**](#scrollTo=E3sSwzshfSpE) to our data preprocessing configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bj-4l7Qvmmx_",
    "outputId": "1d53a36e-697d-47a8-9182-1234568c41eb"
   },
   "outputs": [],
   "source": [
    "# Adding rescale, rotation_range, width_shift_range, height_shift_range,\n",
    "# shear_range, zoom_range, and horizontal flip to our ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Flow training images in batches of 32 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        \"train\",  # This is the source directory for training images\n",
    "        target_size=(150, 150),  # All images will be resized to 150x150\n",
    "        batch_size=20,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "# Flow validation images in batches of 32 using val_datagen generator\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "        'validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjHYQ8BNmmyA"
   },
   "source": [
    "If we train a new network using this data augmentation configuration, our network will never see the same input twice. However the inputs that it sees are still heavily intercorrelated, so this might not be quite enough to completely get rid of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpVLH77tmmyA",
    "outputId": "abd114e2-eab9-4a40-82d6-1ad5fdb1d979"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(150, 150, 3))\n",
    "x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(inputs)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "x = tf.keras.layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "x = tf.keras.layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MdnRta-mmyA"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSqInrBAmmyA",
    "outputId": "70c4d55b-8078-4531-f192-feec8abe7b59"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "3SSLowCRmmyB",
    "outputId": "7231fd8a-6fce-4ad8-ca37-5afa8300ba2c"
   },
   "outputs": [],
   "source": [
    "diagnostic_learning_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNYxL6a2mmyB"
   },
   "source": [
    "## Adding Dropout\n",
    "\n",
    "Another popular strategy for fighting overfitting is to use **dropout**\n",
    "Let's reconfigure our convnet architecture from Exercise 1 to add some dropout, right before the final classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edtVtIBemmyB",
    "outputId": "55151a6c-c93b-4b7f-f0cd-4de94936a54d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Our input feature map is 150x150x3: 150x150 for the image pixels, and 3 for\n",
    "# the three color channels: R, G, and B\n",
    "img_input = layers.Input(shape=(150, 150, 3))\n",
    "\n",
    "# First convolution extracts 16 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(16, 3, activation='relu')(img_input)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Second convolution extracts 32 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Conv2D(32, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Third convolution extracts 64 filters that are 3x3\n",
    "# Convolution is followed by max-pooling layer with a 2x2 window\n",
    "x = layers.Convolution2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "# Flatten feature map to a 1-dim tensor\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Create a fully connected layer with ReLU activation and 512 hidden units\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Create output layer with a single node and sigmoid activation\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Configure and compile the model\n",
    "model = Model(img_input, output)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=0.001),\n",
    "              metrics=['acc'])\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kFUXYxrmmyB"
   },
   "outputs": [],
   "source": [
    "## Retrain the Model\n",
    "\n",
    "# With data augmentation and dropout in place, let's retrain our convnet model. This time, let's train on all 2,000 images available, for 30 epochs, and validate on all 1,000 validation images. (This may take a few minutes to run.) See if you can write the code yourself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txCaGAmimmyB"
   },
   "source": [
    "WRITE CODE TO TRAIN THE MODEL ON ALL 2000 IMAGES FOR 30 EPOCHS, AND VALIDATE \n",
    "ON ALL 1,000 VALIDATION IMAGES\n",
    "\n",
    "### Solution\n",
    "\n",
    "Click below for the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORw-NetUmmyC",
    "outputId": "aa39dd26-db0e-43b9-d169-8ff06512d3c8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50,  # 1000 images = batch_size * steps\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZHAAt0_mmyC"
   },
   "source": [
    "Note that with data augmentation in place, the 2,000 training images are randomly transformed each time a new training epoch runs, which means that the model will never see the same image twice during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRFnXvgammyC"
   },
   "source": [
    "## Evaluate the Results\n",
    "\n",
    "Let's evaluate the results of model training with data augmentation and dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "_KkVgPRfmmyC",
    "outputId": "9939593f-0d35-4a2e-8827-fca75d92fffd"
   },
   "outputs": [],
   "source": [
    "diagnostic_learning_curves() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hqeGuRSmmyC"
   },
   "source": [
    "Much better! We are no longer overfitting, and we have gained ~3 validation accuracy percentage points (see the green line in the top chart). In fact, judging by our training profile, we could keep fitting our model for 30+ more epochs and we could probably get to ~80%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4Z7ARLymmyC",
    "outputId": "426fcbe1-efd6-4bf1-b13a-14ed189c6ffd"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.evaluate(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kIzXUb6RmmyD"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "# y_proba = model.predict(validation_generator) \n",
    "y_proba\n",
    "# pd.Series(y_proba.flatten()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XK_FO0ammyD"
   },
   "outputs": [],
   "source": [
    "# y_pred = (y_proba > 0.5).astype('int32')\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "id": "EeHaBPEemmyD",
    "outputId": "214abbae-0178-4af2-820b-a40085ef20b3"
   },
   "outputs": [],
   "source": [
    "class_names=['cat','dog']\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(validation_generator.classes, y_pred)\n",
    "conf_mat\n",
    "plot_confusion_matrix(conf_mat=conf_mat,\n",
    "                                colorbar=True,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True\n",
    "                                # class_names=class_names\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iddI_uka_gV0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Colab_catsVsDogs.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
